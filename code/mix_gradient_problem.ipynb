{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is available\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import torch\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import drjit as dr\n",
    "import mitsuba as mi\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import clear_output\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from convolutions import *\n",
    "from utils_fns import *\n",
    "from utils_general import update_sigma_linear, run_scheduler_step, plt_errors, show_with_error\n",
    "from optimizations import *\n",
    "from utils_optim import run_optimization, run_grad_optimization, run_cg_optimization, run_bfgs_optimization\n",
    "from utils_general import run_scheduler_step\n",
    "from utils_mitsuba import get_mts_rendering, render_smooth\n",
    "from read_scenes import create_scene_from_xml\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(\"is available\")\n",
    "    mi.set_variant('cuda_ad_rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup f(x), g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_f(weights, x):\n",
    "    '''\n",
    "    Analytic, for linear 3d x -> 4d output\n",
    "    weight matrix with shape (4,3)\n",
    "    '''\n",
    "    return weights @ x\n",
    "    \n",
    "def loss_f(input):\n",
    "    '''\n",
    "    Loss function, s.t. second derivative is non-zero\n",
    "    '''\n",
    "    return torch.sum(input**4)\n",
    "    \n",
    "def f(weights, x):\n",
    "    return loss_f(layer_f(weights, x))\n",
    "\n",
    "def ddloss_f(input):\n",
    "    '''\n",
    "    Second derivative of loss function wrt input\n",
    "    '''\n",
    "    return 12*input**2\n",
    "\n",
    "def dlayer_f(x):\n",
    "    '''\n",
    "    Derivative of layer_f wrt weights\n",
    "    each column would be x with shape (4,3)\n",
    "    '''\n",
    "    x = x.reshape(1,-1)\n",
    "    return x.expand(4,3)\n",
    "# def analytical_dfdx(x):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([[6.],\n",
      "        [6.],\n",
      "        [6.],\n",
      "        [6.]], device='cuda:0', grad_fn=<MmBackward0>)\n",
      "loss: 5184.0\n",
      "d2loss/dy2: tensor([[432.],\n",
      "        [432.],\n",
      "        [432.],\n",
      "        [432.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "dy/dx: tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.],\n",
      "        [1., 2., 3.],\n",
      "        [1., 2., 3.]], device='cuda:0', grad_fn=<ExpandBackward0>)\n",
      "analytical second order derivatives for weights: tensor([[6048., 6048., 6048., 6048.],\n",
      "        [6048., 6048., 6048., 6048.],\n",
      "        [6048., 6048., 6048., 6048.],\n",
      "        [6048., 6048., 6048., 6048.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor([[[[ 432.,  864., 1296.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.]],\n",
      "\n",
      "         [[ 864., 1728., 2592.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.]],\n",
      "\n",
      "         [[1296., 2592., 3888.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.]]],\n",
      "\n",
      "\n",
      "        [[[   0.,    0.,    0.],\n",
      "          [ 432.,  864., 1296.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.]],\n",
      "\n",
      "         [[   0.,    0.,    0.],\n",
      "          [ 864., 1728., 2592.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.]],\n",
      "\n",
      "         [[   0.,    0.,    0.],\n",
      "          [1296., 2592., 3888.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.]]],\n",
      "\n",
      "\n",
      "        [[[   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [ 432.,  864., 1296.],\n",
      "          [   0.,    0.,    0.]],\n",
      "\n",
      "         [[   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [ 864., 1728., 2592.],\n",
      "          [   0.,    0.,    0.]],\n",
      "\n",
      "         [[   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [1296., 2592., 3888.],\n",
      "          [   0.,    0.,    0.]]],\n",
      "\n",
      "\n",
      "        [[[   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [ 432.,  864., 1296.]],\n",
      "\n",
      "         [[   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [ 864., 1728., 2592.]],\n",
      "\n",
      "         [[   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [   0.,    0.,    0.],\n",
      "          [1296., 2592., 3888.]]]], device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], device=device, requires_grad=True)\n",
    "x = x.reshape(3,1)\n",
    "weights = torch.rand((4,3), device=device, requires_grad=True)\n",
    "# weights = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]], device=device, requires_grad=True)\n",
    "# weights = torch.tensor([[1.0, 2.0, 3.0], [1.0, 2.0, 3.0], [1.0, 2.0, 3.0], [1.0, 2.0, 3.0]], device=device, requires_grad=True)\n",
    "weights = torch.tensor([[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], device=device, requires_grad=True)\n",
    "y = layer_f(weights, x)\n",
    "loss = loss_f(y)\n",
    "print(f'y: {y}')    \n",
    "print(f'loss: {loss}')\n",
    "\n",
    "ddloss = ddloss_f(y)\n",
    "dlayer = dlayer_f(x)\n",
    "print(f'd2loss/dy2: {ddloss}')\n",
    "print(f'dy/dx: {dlayer}')\n",
    "\n",
    "weights_sod = dlayer @ dlayer.T * ddloss\n",
    "print(f'analytical second order derivatives for weights: {weights_sod}')\n",
    "hess_f = torch.func.hessian(f, argnums=0)\n",
    "# print(weights)\n",
    "print(hess_f(weights, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
