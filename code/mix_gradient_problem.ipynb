{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is available\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os \n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import torch\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import drjit as dr\n",
    "import mitsuba as mi\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import clear_output\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "\n",
    "\n",
    "from convolutions import *\n",
    "from utils_fns import *\n",
    "from utils_general import update_sigma_linear, run_scheduler_step, plt_errors, show_with_error\n",
    "from optimizations import *\n",
    "from utils_optim import run_optimization, run_grad_optimization, run_cg_optimization, run_bfgs_optimization\n",
    "from utils_general import run_scheduler_step\n",
    "from utils_mitsuba import get_mts_rendering, render_smooth\n",
    "from read_scenes import create_scene_from_xml\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(\"is available\")\n",
    "    mi.set_variant('cuda_ad_rgb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup f(x), g(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_f(weights, x):\n",
    "    '''\n",
    "    Analytic, for linear nd x -> md output\n",
    "    '''\n",
    "    return weights @ x\n",
    "    \n",
    "def loss_f(input):\n",
    "    '''\n",
    "    Loss function, s.t. second derivative is non-zero\n",
    "    '''\n",
    "    return torch.sum(input**4)\n",
    "    \n",
    "def f(weights, x):\n",
    "    return loss_f(layer_f(weights, x))\n",
    "\n",
    "def ddloss_f(input):\n",
    "    '''\n",
    "    Second derivative of loss function wrt input\n",
    "    '''\n",
    "    input = input.flatten()\n",
    "    return torch.diag(12*input**2)\n",
    "\n",
    "def dlayer_f(weights, x):\n",
    "    '''\n",
    "    Jacobian of layer_f wrt weights\n",
    "    '''\n",
    "    return torch.func.jacrev(layer_f, argnums=0)(weights, x)\n",
    "\n",
    "def jvp_layer_f(weights, x, v):\n",
    "    '''\n",
    "    JVP of layer_f wrt weights\n",
    "    '''\n",
    "    return torch.func.jvp(layer_f, (weights, x), (v, torch.zeros_like(x)))[1]\n",
    "# def analytical_dfdx(x):\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low dim test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytical second order derivatives for weights: \n",
      "[[ 1.46  4.85  3.96  0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 4.85 16.12 13.16  0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 3.96 13.16 10.75  0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.26  0.85  0.69  0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.85  2.82  2.31  0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.69  2.31  1.88  0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.1   0.35  0.28  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.35  1.15  0.94  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.28  0.94  0.77  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.57  1.89  1.54]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.89  6.27  5.12]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.54  5.12  4.18]]\n",
      "Computed from pytorch: \n",
      "[[ 1.46  4.85  3.96  0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 4.85 16.12 13.16  0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 3.96 13.16 10.75  0.    0.    0.    0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.26  0.85  0.69  0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.85  2.82  2.31  0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.69  2.31  1.88  0.    0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.1   0.35  0.28  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.35  1.15  0.94  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.28  0.94  0.77  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.57  1.89  1.54]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.89  6.27  5.12]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.    1.54  5.12  4.18]]\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], device=device, requires_grad=True)\n",
    "x = torch.rand(3, device=device, requires_grad=True)\n",
    "x = x.reshape(3,1)\n",
    "weights = torch.rand((4,3), device=device, requires_grad=True)\n",
    "# weights = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0], [10.0, 11.0, 12.0]], device=device, requires_grad=True)\n",
    "# weights = torch.tensor([[1.0, 2.0, 3.0], [1.0, 2.0, 3.0], [1.0, 2.0, 3.0], [1.0, 2.0, 3.0]], device=device, requires_grad=True)\n",
    "# weights = torch.tensor([[1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]], device=device, requires_grad=True)\n",
    "y = layer_f(weights, x)\n",
    "loss = loss_f(y)\n",
    "# print(f'y: {y}')    \n",
    "# print(f'loss: {loss}')\n",
    "\n",
    "ddloss = ddloss_f(y)\n",
    "dlayer = dlayer_f(weights, x).reshape(4,12)\n",
    "weights_sod =  dlayer.T @ ddloss @ dlayer\n",
    "# print(f'd2loss/dy2: \\n{ddloss.cpu().detach().numpy()}')\n",
    "# print(f'dy/dx: \\n{dlayer.cpu().detach().numpy()}')\n",
    "\n",
    "hess_f = torch.func.hessian(f, argnums=0)\n",
    "weights_sod_torch = hess_f(weights, x).reshape(12,12)\n",
    "with np.printoptions(linewidth=np.inf, precision=2):\n",
    "    print(f'analytical second order derivatives for weights: \\n{weights_sod.cpu().detach().numpy()}')\n",
    "    print(f'Computed from pytorch: \\n{weights_sod_torch.cpu().detach().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hessian-vector product: [ 9.21 30.63 25.01  0.96  3.19  2.6   0.38  1.28  1.04  2.23  7.42  6.06]\n",
      "Analytical Hessian-vector product: [ 9.21 30.63 25.01  0.96  3.19  2.6   0.38  1.28  1.04  2.23  7.42  6.06]\n"
     ]
    }
   ],
   "source": [
    "# torch hvp\n",
    "random_vec = torch.rand(12, device=device)\n",
    "hvp_torch = weights_sod_torch @ random_vec\n",
    "\n",
    "# analytical hvp\n",
    "random_vec = random_vec.reshape(4,3)\n",
    "jvp_torch = jvp_layer_f(weights, x, random_vec)\n",
    "jvp_analytical = dlayer @ random_vec.flatten()\n",
    "hvp_analytical = dlayer.T @ ddloss @ jvp_torch\n",
    "with np.printoptions(linewidth=np.inf, precision=2):\n",
    "    # print(f'random vec: {random_vec.cpu().detach().numpy()}')\n",
    "    print(f'Hessian-vector product: {hvp_torch.cpu().detach().numpy()}')\n",
    "    # print(f'Jacobian-vector product torch: {jvp_torch.cpu().detach().numpy()}')\n",
    "    # print(f'Analytical Jacobian-vector product: {jvp_analytical.cpu().detach().numpy()}')\n",
    "    print(f'Analytical Hessian-vector product: {hvp_analytical.cpu().detach().numpy().flatten()}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High dim test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For input shape 1000 and output shape 5:\n",
      "Different element between analytical and torch HVP: tensor([], device='cuda:0', dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "input_shape = 1000\n",
    "output_shape = 5\n",
    "\n",
    "x = torch.rand((input_shape,1), device=device, requires_grad=True)\n",
    "weights = torch.rand((output_shape, input_shape), device=device, requires_grad=True)\n",
    "y = layer_f(weights, x)\n",
    "loss = loss_f(y)\n",
    "\n",
    "ddloss = ddloss_f(y)\n",
    "dlayer = dlayer_f(weights, x).reshape(output_shape, input_shape*output_shape)\n",
    "\n",
    "hess_f = torch.func.hessian(f, argnums=0)\n",
    "weights_sod_torch = hess_f(weights, x).reshape(input_shape*output_shape,input_shape*output_shape)\n",
    "\n",
    "# torch hvp\n",
    "random_vec = torch.rand(input_shape*output_shape, device=device)\n",
    "hvp_torch = weights_sod_torch @ random_vec\n",
    "\n",
    "# analytical hvp\n",
    "random_vec = random_vec.reshape(output_shape, input_shape)\n",
    "jvp_torch = jvp_layer_f(weights, x, random_vec)\n",
    "hvp_analytical = dlayer.T @ ddloss @ jvp_torch\n",
    "\n",
    "hvp_analytical = hvp_analytical.flatten()\n",
    "hvp_torch = hvp_torch.flatten()\n",
    "close_elements = torch.isclose(hvp_analytical.flatten(), hvp_torch.flatten())\n",
    "not_close_indices = torch.where(~close_elements)[0]\n",
    "\n",
    "print(f'For input shape {input_shape} and output shape {output_shape}:')\n",
    "print(f'Different element between analytical and torch HVP: {not_close_indices}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## real task size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_size_from_error(error):\n",
    "    allocation_pattern = r\"Tried to allocate ([\\d.]+ GiB)\"\n",
    "    allocation_match = re.search(allocation_pattern, str(error))\n",
    "    if allocation_match:\n",
    "        return allocation_match.group(1)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For input shape 196608 and output shape 10 (a likely task size):\n",
      "CUDA out of memory(14400.00 GiB needed) for brute force Hessian computation\n",
      "Analytical HVP computed successfully\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache() # clears cache for large matrix\n",
    "input_shape = 256*256*3\n",
    "output_shape = 10\n",
    "\n",
    "x = torch.rand((input_shape,1), device=device, requires_grad=True)\n",
    "weights = torch.rand((output_shape, input_shape), device=device, requires_grad=True)\n",
    "# torch hvp\n",
    "random_vec = torch.rand(input_shape*output_shape, device=device)\n",
    "\n",
    "print(f'For input shape {input_shape} and output shape {output_shape} (a likely task size):')\n",
    "\n",
    "# try block for hessian torch with size of mug task\n",
    "try:    \n",
    "    hess_f = torch.func.hessian(f, argnums=0)\n",
    "    weights_sod_torch = hess_f(weights, x).reshape(input_shape*output_shape,input_shape*output_shape)\n",
    "    hvp_torch = weights_sod_torch @ random_vec\n",
    "    hvp_torch = hvp_torch.flatten()\n",
    "    print(f'Torch HVP computed successfully')\n",
    "except RuntimeError as e:\n",
    "    if 'out of memory' in str(e):\n",
    "        size = get_memory_size_from_error(e)\n",
    "        print(f\"CUDA out of memory({size} needed) for brute force Hessian computation\")\n",
    "\n",
    "# try block for H(theta)VP torch with size of mug task\n",
    "try:\n",
    "    y = layer_f(weights, x)\n",
    "    loss = loss_f(y)\n",
    "    ddloss = ddloss_f(y)\n",
    "    dlayer = dlayer_f(weights, x).reshape(output_shape, input_shape*output_shape)\n",
    "    # analytical hvp \n",
    "    random_vec = random_vec.reshape(output_shape, input_shape)\n",
    "    jvp_torch = jvp_layer_f(weights, x, random_vec)\n",
    "    hvp_analytical = dlayer.T @ ddloss @ jvp_torch\n",
    "    hvp_analytical = hvp_analytical.flatten()\n",
    "    print(f'Analytical HVP computed successfully')\n",
    "except RuntimeError as e:\n",
    "    if 'out of memory' in str(e):\n",
    "        size = get_memory_size_from_error(e)\n",
    "        print(f\"CUDA out of memory({size} needed) for HVP computation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
